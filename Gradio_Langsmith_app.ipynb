{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033237df-c8f4-4c07-b0ac-32c378bdcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from app import ChatBot, ChatBotConfig\n",
    "from typing import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0a5218",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LANGCHAIN_TRACING_V2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m load_dotenv()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mLANGCHAIN_TRACING_V2\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(LANGCHAIN_PROJECT)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(LANGCHAIN_ENDPOINT)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LANGCHAIN_TRACING_V2' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "515f42ca-2137-4e16-a1b5-c1dbdbbe176a",
   "metadata": {},
   "source": [
    "Langsmith API key\n",
    "ls__3119e6b5468c48268f70740d877021b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8078bd7c-1ede-49d5-ac38-0326dd290484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langsmith\n",
    "\n",
    "from langsmith import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23c8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b70983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://98bafa29894742aa62.gradio.componentsoft.ai\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://98bafa29894742aa62.gradio.componentsoft.ai\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "username = \"Ericsson\"\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"GPT\": get_args(ChatBotConfig.OPENAI_MODELS),\n",
    "    \"Mistral\": get_args(ChatBotConfig.MISTRAL_MODELS),\n",
    "    \"Llama\": get_args(ChatBotConfig.LLAMA_MODELS),\n",
    "}\n",
    "\n",
    "system_prompt = []\n",
    "temperature = []\n",
    "max_tokens = []\n",
    "ChatBot.store = {}\n",
    "\n",
    "def exec_prompt(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}})\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    return history_pairs, \"\" \n",
    "\n",
    "def exec_prompt_streaming(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "    response = chain.stream({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}})\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    history_pairs.append([question, \"\"])\n",
    "    for res in response:\n",
    "        if res is not None:\n",
    "            history_pairs[-1][1] += res\n",
    "        yield history_pairs, \"\"         \n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"LangSmith\") as demo:\n",
    "    session_id = gr.Textbox(value = uuid.uuid4, interactive=False, visible=False)\n",
    "    gr.Markdown(\"# Component Soft 5G RAG Demo with LangSmith\")\n",
    "    #system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"Mistral\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"Mistral\"]), label=\"Model\", value=\"mistral-large\")       \n",
    "        \"\"\"temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\"\"\"\n",
    "    with gr.Row():\n",
    "        chatbot=gr.Chatbot(label=\"CompSoft_5G_RAG\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    clear_btn.click(lambda session_id: ChatBot.del_session_history(username, session_id), [session_id], None, preprocess=False)\n",
    "\n",
    "    callback.setup([modelfamily, model, chatbot], \"flagged_data_points\")\n",
    "    flag_btn.click(lambda *args: callback.flag(args), [modelfamily, model, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is 5G?\", \"What are the main adventages of 5G compared to 4G?\", \"What frequencies does 5G use?\",  \"Which organisations are responsible for the standardization of 5G?\", \n",
    "         \"What is OFDMA?\", \"What is the difference between OFDMA and OFDM?\", \"What are the main components of 5G core networks?\", \"What were the design principles of Massive MTC?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, state_session_capacity=20)\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5548e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
