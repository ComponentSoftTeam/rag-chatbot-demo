{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce769a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Tuple\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f2d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import ConfigurableFieldSpec, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e3ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rconsole/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#from rag_utils import RAG, EmbeddingType, RagResponse\n",
    "from rag_utils import RAG, EmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd455e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ANSWER_PROMPT, CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963d54bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b6a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    VECTOR_STORES = {\n",
    "        EmbeddingType.OPEN_AI: RAG(embedding_type=EmbeddingType.OPEN_AI),\n",
    "        EmbeddingType.SENTENCE_TRANSFORMER: RAG(embedding_type=EmbeddingType.SENTENCE_TRANSFORMER),\n",
    "        EmbeddingType.MISTRAL: RAG(embedding_type=EmbeddingType.MISTRAL),\n",
    "    }\n",
    "\n",
    "    store: Dict[Tuple[str, str], BaseChatMessageHistory] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "        if (user_id, conversation_id) not in ChatBot.store:\n",
    "            ChatBot.store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "        return ChatBot.store[(user_id, conversation_id)]\n",
    "\n",
    "    @classmethod\n",
    "    def format_document(cls, doc: Document) -> str:\n",
    "        # base_info = {\"page_content\": doc.page_content, **doc.metadata}\n",
    "        # document_info = {k: base_info[k] for k in DOCUMENT_PROMPT.input_variables}\n",
    "        document_info = {\n",
    "            \"page_content\": doc.page_content,\n",
    "            # \"file_source\": \"data.pdf, page 5\",\n",
    "        }\n",
    "        return DOCUMENT_PROMPT.format(**document_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_documents(docs, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [ChatBot.format_document(doc) for doc in docs]\n",
    "        context = document_separator.join(doc_strings)\n",
    "        return context or \"No relevant context found.\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_vector_store(cls, embedding_type: EmbeddingType) -> VectorStore:\n",
    "        return cls.VECTOR_STORES[embedding_type]\n",
    "\n",
    "    @classmethod\n",
    "    def construct_chain(cls, embedding_type: EmbeddingType, model_type: str):\n",
    "        vector_store = cls.get_vector_store(embedding_type)\n",
    "        retriever = vector_store.as_retriever()\n",
    "\n",
    "        #####################################x\n",
    "        # INPUTS:                            #\n",
    "        #   chat_history: ChatMessageHistory #\n",
    "        #   question: str                    #\n",
    "        # OUTPUTS:                           #\n",
    "        #   standalone_question: str         #\n",
    "        #   chat_history: str                #\n",
    "        #   question: str                    #\n",
    "        #####################################x\n",
    "        standalone_question = RunnableParallel(\n",
    "            standalone_question=RunnablePassthrough.assign(chat_history=lambda x: get_buffer_string(x[\"chat_history\"]))\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | ChatOpenAI(temperature=0)\n",
    "            | StrOutputParser(),\n",
    "            question=lambda x: x[\"question\"],\n",
    "            chat_history=lambda x: x[\"chat_history\"],\n",
    "        )\n",
    "\n",
    "        #########################################\n",
    "        # INPUTS:                               #\n",
    "        #   standalone_question: str            #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        # OUTPUTS:                              #\n",
    "        #   context: str                        #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        #########################################\n",
    "        question_context = {\n",
    "            \"context\": (\n",
    "                itemgetter(\"standalone_question\")\n",
    "                | retriever\n",
    "                | cls.combine_documents\n",
    "            ),\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        # INPUTS:                                #\n",
    "        #   chat_history: ChatMessageHistory     #\n",
    "        #   question: str                        #\n",
    "        # OUTPUTS:                               #\n",
    "        #   str (Model answer)                   #\n",
    "        ##########################################\n",
    "        rag_chain = (\n",
    "            standalone_question\n",
    "            | question_context\n",
    "            | ANSWER_PROMPT\n",
    "            | ChatOpenAI()\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        ################################\n",
    "        # INPUTS:                      #\n",
    "        #   question: str              #\n",
    "        #   configuration: dict        #\n",
    "        #      user_id: str            #\n",
    "        #      conversation_id: str    #\n",
    "        # OUTPUTS:                     #\n",
    "        #   str                        #\n",
    "        ################################\n",
    "        # NOTE: The user_id and conversation_id as a pair defines the session and thus the chat history\n",
    "        with_message_history = RunnableWithMessageHistory(\n",
    "            rag_chain,\n",
    "            get_session_history=ChatBot.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            history_factory_config=[\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"user_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"User ID\",\n",
    "                    description=\"Unique identifier for the user.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"conversation_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"Conversation ID\",\n",
    "                    description=\"Unique identifier for the conversation.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return with_message_history\n",
    "    \n",
    "\n",
    "    chains: Dict[Tuple[EmbeddingType, str], RunnableLambda] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_chain(cls, embedding_type, model_type):\n",
    "        if not cls.chains:\n",
    "            cls.chains = {\n",
    "                (EmbeddingType.OPEN_AI, \"openai\"): ChatBot.construct_chain(EmbeddingType.OPEN_AI, \"openai\"),\n",
    "            } \n",
    "\n",
    "        return cls.chains[(embedding_type, model_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438bdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "user_config = {\n",
    "    \"user_id\": \"user_id\",\n",
    "    \"conversation_id\": uuid.uuid4().hex,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a6943-5f2a-4f68-8a3f-10bbdc9c54ad",
   "metadata": {},
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "trace = {\n",
    "    \"callbacks\": [\n",
    "        CallbackHandler(\n",
    "            secret_key=\"sk-lf-de74539a-7177-49a8-9c3e-50492e90d1a9\",\n",
    "            public_key=\"pk-lf-71d6d1c4-7a5b-4f46-b75c-cd8dad7e35ff\",\n",
    "            host=\"http://localhost:3000\",\n",
    "        )\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0863c461-7c02-4206-8a93-a58bc01af297",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3967513453.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(message, end=\"\")\"\"\"\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "ChatBot.store = {}\n",
    "while True:\n",
    "    # print history\n",
    "    history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "    print(f\"history = {history}\\n\\n\")\n",
    "\n",
    "    # get question\n",
    "    question = input(\"Enter your question: \")\n",
    "    if question == \"exit\": break\n",
    "\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    \n",
    "    response = chain.stream({\"question\": question}, config={\"configurable\": user_config})\n",
    "\n",
    "    for message in response:\n",
    "        print(message, end=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bac66896-8067-4298-8de0-afc558d0c63e",
   "metadata": {},
   "source": [
    "while True:\n",
    "    # print history\n",
    "    for user, message in ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"]):\n",
    "        print(f\"user: {user}, message: {message}\")\n",
    "\n",
    "    # get question\n",
    "    question = input(\"Enter your question: \")\n",
    "    if question == \"exit\": break\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": user_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b70983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"OpenAI\": [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "    \"Mistral\": [\"mistral-tiny\", \"mistral-small\", \"mistral-medium\", \"mistral-large\"],\n",
    "    \"Llama\": [\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\"],\n",
    "}\n",
    "\n",
    "system_prompt = []\n",
    "temperature = []\n",
    "max_tokens = []\n",
    "ChatBot.store = {}\n",
    "\n",
    "def exec_prompt(chatbot, question, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    #response = chain.invoke({\"question\": question}, config={\"configurable\": user_config} | trace)\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": user_config})\n",
    "    ChatBot_history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "    history = []\n",
    "    for _, chat in ChatBot_history:\n",
    "        \"\"\"print(_)\n",
    "        print(chat)\n",
    "        print(\"\\n\\n\\n\")\"\"\"\n",
    "        for i, chat in enumerate(chat):\n",
    "            print(f\"chat = {chat}\")\n",
    "            if i % 2 == 0:\n",
    "                chat_str = str(chat)\n",
    "                new_round = [chat_str[len(\"content='\"):-1]]\n",
    "                history.append(new_round)\n",
    "                print(f\"i={i}, text={new_round}, history={history}\")\n",
    "            else:\n",
    "                chat_str = str(chat)\n",
    "                new_round = new_round.append(chat_str[len(\"content='\"):-1])\n",
    "\n",
    "    #print(history)\n",
    "    return history, \"\" \n",
    "\n",
    "\"\"\"def exec_prompt_streaming(chatbot, prompt, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)\n",
    "    Prompt.set_model(model_family, model)\n",
    "    \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    stream = Prompt.exec_streaming(chat_history)\n",
    "    for new_token in stream:\n",
    "        if new_token is not None:\n",
    "            chat_history[-1][1] += str(new_token)\n",
    "        yield chat_history, \"\" \"\"\"\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    gr.Markdown(\"# Component Soft 5G RAG Demo\")\n",
    "    #system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"OpenAI\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"OpenAI\"]), label=\"Model\", value=\"gpt-3.5-turbo\")       \n",
    "        \"\"\"temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\"\"\"\n",
    "    with gr.Row():\n",
    "        #chatbot=gr.Textbox(label=\"CompSoft_5G_RAG\", lines=10, max_lines=20, show_copy_button=True)\n",
    "        chatbot=gr.Chatbot(label=\"CompSoft_5G_RAG\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        #submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        #flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    #submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot, prompt, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot, prompt, modelfamily, model], outputs=[chatbot, prompt])\n",
    "\n",
    "    #callback.setup([system_prompt, modelfamily, model, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "    #flag_btn.click(lambda *args: callback.flag(args), [system_prompt, modelfamily, model, temperature, max_tokens, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is 5G?\", \"What are the main adventages of 5G compared to 4G?\", \"What frequencies does 5G use?\", \"What is OFDMA?\", \n",
    "         \"Which organisations are responsible for the standardization of 5G?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"/home/rconsole/GIT/AI-434/source/labfiles/data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dd492-fe68-4933-8599-389464c1eef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
