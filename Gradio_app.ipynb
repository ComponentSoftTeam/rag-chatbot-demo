{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce769a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmskss/Development/rag-chatbot-demo/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from app import ChatBot, ChatBotConfig\n",
    "from typing import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0a5218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31f59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "  secret_key=\"sk-lf-5661f1bf-a313-4291-a5ee-a40acf2f6073\",\n",
    "  public_key=\"pk-lf-2e5389e9-580f-482e-bff5-c6845259eac1\",\n",
    "  host=\"http://localhost:3000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0516de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "trace = {\n",
    "        \"callbacks\": [\n",
    "            CallbackHandler(\n",
    "                secret_key=\"sk-lf-5661f1bf-a313-4291-a5ee-a40acf2f6073\",\n",
    "                public_key=\"pk-lf-2e5389e9-580f-482e-bff5-c6845259eac1\",\n",
    "                host=\"http://localhost:3000\",\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23c8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b70983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://ae815ff8c221df64a9.gradio.componentsoft.ai\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ae815ff8c221df64a9.gradio.componentsoft.ai\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmskss/Development/rag-chatbot-demo/venv/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "username = \"Ericsson\"\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"GPT\": get_args(ChatBotConfig.OPENAI_MODELS),\n",
    "    \"Mistral\": get_args(ChatBotConfig.MISTRAL_MODELS),\n",
    "    \"Llama\": get_args(ChatBotConfig.LLAMA_MODELS),\n",
    "}\n",
    "\n",
    "system_prompt = []\n",
    "temperature = []\n",
    "max_tokens = []\n",
    "ChatBot.store = {}\n",
    "\n",
    "def exec_prompt(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}} | trace)\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    return history_pairs, \"\" \n",
    "\n",
    "def exec_prompt_streaming(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "    response = chain.stream({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}}| trace)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    history_pairs.append([question, \"\"])\n",
    "    for res in response:\n",
    "        if res is not None:\n",
    "            history_pairs[-1][1] += res\n",
    "        yield history_pairs, \"\"         \n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    session_id = gr.Textbox(value = uuid.uuid4, interactive=False, visible=False)\n",
    "    gr.Markdown(\"# Component Soft 5G RAG Demo\")\n",
    "    #system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"GPT\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"GPT\"]), label=\"Model\", value=\"gpt-3.5-turbo\")       \n",
    "        \"\"\"temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\"\"\"\n",
    "    with gr.Row():\n",
    "        chatbot=gr.Chatbot(label=\"CompSoft_5G_RAG\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        #flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    clear_btn.click(lambda session_id: ChatBot.del_session_history(username, session_id), [session_id], None, preprocess=False)\n",
    "\n",
    "    #callback.setup([system_prompt, modelfamily, model, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "    #flag_btn.click(lambda *args: callback.flag(args), [system_prompt, modelfamily, model, temperature, max_tokens, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is 5G?\", \"What are the main adventages of 5G compared to 4G?\", \"What frequencies does 5G use?\", \"What is OFDMA?\", \n",
    "         \"Which organisations are responsible for the standardization of 5G?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, state_session_capacity=20)\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"/home/rconsole/GIT/AI-434/source/labfiles/data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5548e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
