{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce769a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Tuple\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f2d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import ConfigurableFieldSpec, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e3ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import RAG, EmbeddingType, RagResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd455e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ANSWER_PROMPT, CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963d54bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3911738",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "VECTOR_STORES = {\n",
    "    EmbeddingType.OPEN_AI: RAG(embedding_type=EmbeddingType.OPEN_AI),\n",
    "    EmbeddingType.SENTENCE_TRANSFORMER: Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings()),\n",
    "    EmbeddingType.MISTRAL: Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b6a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    store: Dict[Tuple[str, str], BaseChatMessageHistory] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "        if (user_id, conversation_id) not in ChatBot.store:\n",
    "            ChatBot.store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "        return ChatBot.store[(user_id, conversation_id)]\n",
    "\n",
    "    @classmethod\n",
    "    def format_document(cls, doc: Document) -> str:\n",
    "        # base_info = {\"page_content\": doc.page_content, **doc.metadata}\n",
    "        # document_info = {k: base_info[k] for k in DOCUMENT_PROMPT.input_variables}\n",
    "        document_info = {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"file_source\": \"data.pdf, page 5\",\n",
    "        }\n",
    "        return DOCUMENT_PROMPT.format(**document_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_documents(docs, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [ChatBot.format_document(doc) for doc in docs]\n",
    "        return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_vector_store(cls, embedding_type: EmbeddingType) -> VectorStore:\n",
    "        return VECTOR_STORES[embedding_type]\n",
    "\n",
    "    @classmethod\n",
    "    def construct_chain(cls, embedding_type: EmbeddingType, model_type: str):\n",
    "        vector_store = cls.get_vector_store(embedding_type)\n",
    "        retriever = vector_store.as_retriever()\n",
    "\n",
    "        #####################################x\n",
    "        # INPUTS:                            #\n",
    "        #   chat_history: ChatMessageHistory #\n",
    "        #   question: str                    #\n",
    "        # OUTPUTS:                           #\n",
    "        #   standalone_question: str         #\n",
    "        #   chat_history: str                #\n",
    "        #   question: str                    #\n",
    "        #####################################x\n",
    "        standalone_question = RunnableParallel(\n",
    "            standalone_question=RunnablePassthrough.assign(chat_history=lambda x: get_buffer_string(x[\"chat_history\"]))\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | ChatOpenAI(temperature=0)\n",
    "            | StrOutputParser(),\n",
    "            question=lambda x: x[\"question\"],\n",
    "            chat_history=lambda x: x[\"chat_history\"],\n",
    "        )\n",
    "\n",
    "        #########################################\n",
    "        # INPUTS:                               #\n",
    "        #   standalone_question: str            #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        # OUTPUTS:                              #\n",
    "        #   context: str                        #\n",
    "        #   standalone_question: str            #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        #########################################\n",
    "        question_context = {\n",
    "            \"context\": (\n",
    "                itemgetter(\"standalone_question\")\n",
    "                | retriever\n",
    "                | cls.combine_documents\n",
    "            ),\n",
    "            \"standalone_question\": lambda x: x[\"standalone_question\"],\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        # INPUTS:                                #\n",
    "        #   chat_history: ChatMessageHistory     #\n",
    "        #   question: str                        #\n",
    "        # OUTPUTS:                               #\n",
    "        #   str (Model answer)                   #\n",
    "        ##########################################\n",
    "        rag_chain = (\n",
    "            standalone_question\n",
    "            | question_context\n",
    "            | ANSWER_PROMPT\n",
    "            | ChatOpenAI()\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        ################################\n",
    "        # INPUTS:                      #\n",
    "        #   question: str              #\n",
    "        #   configuration: dict        #\n",
    "        #      user_id: str            #\n",
    "        #      conversation_id: str    #\n",
    "        # OUTPUTS:                     #\n",
    "        #   str                        #\n",
    "        ################################\n",
    "        # NOTE: The user_id and conversation_id as a pair defines the session and thus the chat history\n",
    "        with_message_history = RunnableWithMessageHistory(\n",
    "            rag_chain,\n",
    "            get_session_history=ChatBot.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            history_factory_config=[\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"user_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"User ID\",\n",
    "                    description=\"Unique identifier for the user.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"conversation_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"Conversation ID\",\n",
    "                    description=\"Unique identifier for the conversation.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return with_message_history\n",
    "    \n",
    "\n",
    "    chains: Dict[Tuple[EmbeddingType, str], RunnableLambda] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_chain(cls, embedding_type, model_type):\n",
    "        if not cls.chains:\n",
    "            cls.chains = {\n",
    "                (EmbeddingType.OPEN_AI, \"openai\"): ChatBot.construct_chain(EmbeddingType.OPEN_AI, \"openai\"),\n",
    "            } \n",
    "\n",
    "        return cls.chains[(embedding_type, model_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438bdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config = {\n",
    "    \"user_id\": \"user_id\",\n",
    "    \"conversation_id\": \"conversation_id\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f60fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a6943-5f2a-4f68-8a3f-10bbdc9c54ad",
   "metadata": {},
   "source": [
    "trace = {\n",
    "    \"callbacks\": [\n",
    "        CallbackHandler(\n",
    "            secret_key=\"sk-lf-de74539a-7177-49a8-9c3e-50492e90d1a9\",\n",
    "            public_key=\"pk-lf-71d6d1c4-7a5b-4f46-b75c-cd8dad7e35ff\",\n",
    "            host=\"http://localhost:3000\",\n",
    "        )\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "160452e7-6aa0-4551-b984-22c799bbae66",
   "metadata": {},
   "source": [
    "while True:\n",
    "    # print history\n",
    "    history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "    print(history)\n",
    "\n",
    "    # get question\n",
    "    question = input(\"Enter your question: \")\n",
    "\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": user_config})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bac66896-8067-4298-8de0-afc558d0c63e",
   "metadata": {},
   "source": [
    "while True:\n",
    "    # print history\n",
    "    for user, message in ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"]):\n",
    "        print(f\"user: {user}, message: {message}\")\n",
    "\n",
    "    # get question\n",
    "    question = input(\"Enter your question: \")\n",
    "    if question == \"exit\": break\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": user_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b70983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://328f42c1d06d9ad002.gradio.componentsoft.ai\n",
      "\n",
      "This share link expires in 168 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://328f42c1d06d9ad002.gradio.componentsoft.ai\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"OpenAI\": [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "    \"Mistral\": [\"mistral-tiny\", \"mistral-small\", \"mistral-medium\", \"mistral-large\"],\n",
    "    \"Llama\": [\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\"],\n",
    "}\n",
    "\n",
    "system_prompt = []\n",
    "temperature = []\n",
    "max_tokens = []\n",
    "ChatBot.store = {}\n",
    "\n",
    "def exec_prompt(chatbot, question, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(embedding_type=EmbeddingType.OPEN_AI, model_type=\"openai\")\n",
    "    #response = chain.invoke({\"question\": question}, config={\"configurable\": user_config} | trace)\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": user_config})\n",
    "    ChatBot_history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "    history = []\n",
    "    for _, chat in ChatBot_history:\n",
    "        \"\"\"print(_)\n",
    "        print(chat)\n",
    "        print(\"\\n\\n\\n\")\"\"\"\n",
    "        for i, chat in enumerate(chat):\n",
    "            print(f\"chat = {chat}\")\n",
    "            if i % 2 == 0:\n",
    "                chat_str = str(chat)\n",
    "                new_round = [chat_str[len(\"content='\"):-1]]\n",
    "                history.append(new_round)\n",
    "                print(f\"i={i}, text={new_round}, history={history}\")\n",
    "            else:\n",
    "                chat_str = str(chat)\n",
    "                new_round = new_round.append(chat_str[len(\"content='\"):-1])\n",
    "\n",
    "    #print(history)\n",
    "    return history, \"\" \n",
    "\n",
    "\"\"\"def exec_prompt_streaming(chatbot, prompt, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)\n",
    "    Prompt.set_model(model_family, model)\n",
    "    \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    stream = Prompt.exec_streaming(chat_history)\n",
    "    for new_token in stream:\n",
    "        if new_token is not None:\n",
    "            chat_history[-1][1] += str(new_token)\n",
    "        yield chat_history, \"\" \"\"\"\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    gr.Markdown(\"# Component Soft 5G RAG Demo\")\n",
    "    #system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"OpenAI\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"OpenAI\"]), label=\"Model\", value=\"gpt-3.5-turbo\")       \n",
    "        \"\"\"temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\"\"\"\n",
    "    with gr.Row():\n",
    "        #chatbot=gr.Textbox(label=\"CompSoft_5G_RAG\", lines=10, max_lines=20, show_copy_button=True)\n",
    "        chatbot=gr.Chatbot(label=\"CompSoft_5G_RAG\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        #submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        #flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    #submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot, prompt, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot, prompt, modelfamily, model], outputs=[chatbot, prompt])\n",
    "\n",
    "    #callback.setup([system_prompt, modelfamily, model, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "    #flag_btn.click(lambda *args: callback.flag(args), [system_prompt, modelfamily, model, temperature, max_tokens, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is 5G?\", \"What are the main adventages of 5G compared to 4G?\", \"What frequencies does 5G use?\", \"What is OFDMA?\", \n",
    "         \"Which organisations are responsible for the standardization of 5G?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"/home/rconsole/GIT/AI-434/source/labfiles/data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dd492-fe68-4933-8599-389464c1eef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
