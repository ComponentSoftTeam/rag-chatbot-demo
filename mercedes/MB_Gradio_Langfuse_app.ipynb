{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033237df-c8f4-4c07-b0ac-32c378bdcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from app import ChatBot, ChatBotConfig\n",
    "from typing import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0a5218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bc838bf-b74d-458f-8b55-c5daa7e3b241",
   "metadata": {},
   "source": [
    "# ! pip install langfuse\n",
    "\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_handler = Langfuse(\n",
    "  secret_key=\"sk-lf-76c2efa1-fd51-4cdf-8429-1397b958b4b2\",\n",
    "  public_key=\"pk-lf-013f7196-3618-41c0-84dd-d8133bd80214\",\n",
    "  host=\"https://cloud.langfuse.com\"\n",
    ")\n",
    "\n",
    "langfuse_handler.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba651e7-4b4a-4e09-bcfb-1a8f14d3df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "trace = {\n",
    "        \"callbacks\": [\n",
    "            CallbackHandler(\n",
    "                secret_key=\"sk-lf-76c2efa1-fd51-4cdf-8429-1397b958b4b2\",\n",
    "                  public_key=\"pk-lf-013f7196-3618-41c0-84dd-d8133bd80214\",\n",
    "                  host=\"https://cloud.langfuse.com\",\n",
    "                  trace_name=\"Mercedes RAG\"\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a23c8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b70983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://b9c1b3ae8fad0d08cc.gradio.componentsoft.ai\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b9c1b3ae8fad0d08cc.gradio.componentsoft.ai\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "username = \"Ericsson\"\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"GPT\": get_args(ChatBotConfig.OPENAI_MODELS),\n",
    "    \"Mistral\": get_args(ChatBotConfig.MISTRAL_MODELS),\n",
    "    \"Llama\": get_args(ChatBotConfig.LLAMA_MODELS),\n",
    "}\n",
    "\n",
    "system_prompt = []\n",
    "temperature = []\n",
    "max_tokens = []\n",
    "ChatBot.store = {}\n",
    "\n",
    "def exec_prompt(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    response = chain.invoke({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}} | trace)\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    return history_pairs, \"\" \n",
    "\n",
    "def exec_prompt_streaming(chatbot, question, session_id, model_family = \"Mistral\", model=\"mistral-large\"):\n",
    "\n",
    "    question = question\n",
    "    # get chain\n",
    "    chain = ChatBot.get_chain(model_family=model_family, model=model)\n",
    "    chat_history = ChatBot.get_session_history(username, session_id)\n",
    "    response = chain.stream({\"question\": question}, config={\"configurable\": {\"user_id\": username, \"conversation_id\": session_id}} | trace)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    history_pairs.append([question, \"\"])\n",
    "    for res in response:\n",
    "        if res is not None:\n",
    "            history_pairs[-1][1] += res\n",
    "        yield history_pairs, \"\"         \n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"LangFuse\") as demo:\n",
    "    session_id = gr.Textbox(value = uuid.uuid4, interactive=False, visible=False)\n",
    "    gr.Markdown(\"# Mercedes RAG Demo with Langfuse\")\n",
    "    #system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"Mistral\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"Mistral\"]), label=\"Model\", value=\"mistral-large\")       \n",
    "        \"\"\"temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\"\"\"\n",
    "    with gr.Row():\n",
    "        chatbot=gr.Chatbot(label=\"Mercedes_RAG\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot, prompt, session_id, modelfamily, model], outputs=[chatbot, prompt])\n",
    "    clear_btn.click(lambda session_id: ChatBot.del_session_history(username, session_id), [session_id], None, preprocess=False)\n",
    "\n",
    "    callback.setup([modelfamily, model, chatbot], \"flagged_data_points\")\n",
    "    flag_btn.click(lambda *args: callback.flag(args), [modelfamily, model, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is 5G?\", \"What are its main adventages compared to 4G?\", \"What frequencies does it use?\",  \"Which organisations are responsible for its standardization?\", \n",
    "         \"What is OFDMA?\", \"What is the difference between OFDMA and OFDM?\", \"What are the main components of 5G core networks?\", \"What were the design principles of Massive MTC?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, state_session_capacity=20)\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5548e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
