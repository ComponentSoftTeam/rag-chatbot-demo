{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1790cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Literal, Tuple, Union, get_args, overload\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81da8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf455671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import ConfigurableFieldSpec, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "#from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63baa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import RAG, EmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65a837d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from prompts import ANSWER_PROMPT, CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa95fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Replicate\n",
    "from langchain_fireworks.chat_models import ChatFireworks\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3556281c-22df-4aba-a42d-a1f1d585751c",
   "metadata": {},
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG Demo\"\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8cc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBotConfig:\n",
    "    MODEL_FAMILY = Literal[\"GPT\", \"Mistral\", \"Llama\"]\n",
    "\n",
    "    OPENAI_MODELS = Literal[\"gpt-3.5-turbo\", \"gpt-4\"]\n",
    "    MISTRAL_MODELS = Literal[\"mistral-tiny\", \"mistral-small\", \"mistral-medium\",  \"mistral-large\"]\n",
    "    __LLAMA_MODEL_VERSIONS = {\n",
    "        #\"llama-2-7b-chat\": \"13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\",\n",
    "        #\"llama-2-13b-chat\": \"f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "        #\"llama-2-70b-chat\": \"02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "        \"llama-2-7b-chat\": \"accounts/fireworks/models/llama-v2-7b-chat\",\n",
    "        \"llama-2-13b-chat\": \"accounts/fireworks/models/llama-v2-13b-chat\",\n",
    "        \"llama-2-70b-chat\": \"accounts/fireworks/models/llama-v2-70b-chat\",\n",
    "        \"llama-3-8b-instruct\": \"accounts/fireworks/models/llama-v3-8b-instruct\",\n",
    "        \"llama-3-70b-instruct\": \"accounts/fireworks/models/llama-v3-70b-instruct\",\n",
    "    }\n",
    "    \n",
    "    LLAMA_MODELS = Literal[\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\", \"llama-3-8b-instruct\", \"llama-3-70b-instruct\"]\n",
    "    MODELS = Union[OPENAI_MODELS, MISTRAL_MODELS, LLAMA_MODELS]\n",
    "\n",
    "    def get_model_name(self, model_family: MODEL_FAMILY, model: MODELS) -> str:\n",
    "        match model_family:\n",
    "            case \"GPT\":\n",
    "                return model\n",
    "            case \"Mistral\":\n",
    "                match model:\n",
    "                    case \"mistral-tiny\":\n",
    "                        return \"mistral-tiny\"\n",
    "                    case \"mistral-small\":\n",
    "                        return \"mistral-small-latest\"\n",
    "                    case \"mistral-medium\":\n",
    "                        return \"mistral-medium-latest\"\n",
    "                    case \"mistral-large\":\n",
    "                        return \"mistral-large-latest\"\n",
    "                    case _: \n",
    "                        raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.MISTRAL_MODELS)}\")\n",
    "            \n",
    "            case \"Llama\":\n",
    "                #return f\"meta/{model}:{ChatBotConfig.__LLAMA_MODEL_VERSIONS[model]}\"\n",
    "                return f\"{ChatBotConfig.__LLAMA_MODEL_VERSIONS[model]}\"\n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"GPT\"], model: OPENAI_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"Mistral\"], model: MISTRAL_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"Llama\"], model: LLAMA_MODELS): ...\n",
    "\n",
    "    def __init__(self, model_family: MODEL_FAMILY, model: MODELS):\n",
    "        self.model_family = model_family\n",
    "        self.model = self.get_model_name(model_family, model)\n",
    "\n",
    "        match model_family:\n",
    "            case \"GPT\":\n",
    "                self.embedding_type = EmbeddingType.OPEN_AI\n",
    "                if model not in get_args(ChatBotConfig.OPENAI_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.OPENAI_MODELS)}\")\n",
    "            case \"Mistral\":\n",
    "                self.embedding_type = EmbeddingType.MISTRAL\n",
    "                if model not in get_args(ChatBotConfig.MISTRAL_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.MISTRAL_MODELS)}\")\n",
    "            case \"Llama\":\n",
    "                self.embedding_type = EmbeddingType.SENTENCE_TRANSFORMER\n",
    "                if model not in get_args(ChatBotConfig.LLAMA_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.LLAMA_MODELS)}\")\n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    #def get_condensation_model(self) -> Union[ChatOpenAI, ChatMistralAI, Replicate]:\n",
    "    def get_condensation_model(self) -> Union[ChatOpenAI, ChatMistralAI, ChatFireworks]:\n",
    "        match self.model_family:\n",
    "            case \"GPT\":\n",
    "                return ChatOpenAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Mistral\":\n",
    "                return ChatMistralAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Llama\":\n",
    "                \"\"\"return Replicate(\n",
    "                    model=self.model,\n",
    "                    model_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 2000, \"prompt_template\": f\"<s>[INST] <<SYS>> {{system_prompt}} <</SYS>>\\n {{prompt}} [/INST]\\n\\nSure, here is a rephrased standalone question based on the original conversation: \"},\n",
    "                )\"\"\"\n",
    "                return ChatFireworks(\n",
    "                    model=self.model,\n",
    "                    temperature=0.02,\n",
    "                    max_tokens=2000,            \n",
    "                )\n",
    "                \n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {self.model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    def get_chat_model(self) -> Union[ChatOpenAI, ChatMistralAI, ChatFireworks]:\n",
    "        match self.model_family:\n",
    "            case \"GPT\":\n",
    "                return ChatOpenAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Mistral\":\n",
    "                return ChatMistralAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Llama\":\n",
    "                return ChatFireworks(\n",
    "                    model=self.model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,            \n",
    "                )\n",
    "                \n",
    "                \"\"\"return Replicate(\n",
    "                    model=self.model,\n",
    "                    model_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 2000, \"prompt_template\": f\"<s>[INST] <<SYS>> {{system_prompt}} <</SYS>>\\n {{prompt}} [/INST]\\n\\nThank you for the question!  \"},\n",
    "                )\"\"\"\n",
    "        \n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {self.model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d97ca9a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    VECTOR_STORES = {\n",
    "        EmbeddingType.OPEN_AI: RAG(embedding_type=EmbeddingType.OPEN_AI),\n",
    "        EmbeddingType.MISTRAL: RAG(embedding_type=EmbeddingType.MISTRAL),\n",
    "        EmbeddingType.SENTENCE_TRANSFORMER: RAG(embedding_type=EmbeddingType.SENTENCE_TRANSFORMER),\n",
    "    }\n",
    "\n",
    "    store: Dict[Tuple[str, str], BaseChatMessageHistory] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "        if (user_id, conversation_id) not in ChatBot.store:\n",
    "            ChatBot.store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "        return ChatBot.store[(user_id, conversation_id)]\n",
    "   \n",
    "    @staticmethod\n",
    "    def del_session_history(user_id: str, conversation_id: str) -> None:\n",
    "        if (user_id, conversation_id) in ChatBot.store:\n",
    "            del ChatBot.store[(user_id, conversation_id)]\n",
    "\n",
    "    @classmethod\n",
    "    def format_document(cls, doc: Document) -> str:\n",
    "        document_info = {\n",
    "            \"content\": doc.page_content,\n",
    "            \"source\": f\"{doc.metadata['source_file']} / Page {doc.metadata['page_num']}\",\n",
    "        }\n",
    "        return DOCUMENT_PROMPT.format(**document_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_documents(docs, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [ChatBot.format_document(doc) for doc in docs]\n",
    "        context = document_separator.join(doc_strings)\n",
    "        return context or \"No relevant context found.\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_vector_store(cls, embedding_type: EmbeddingType) -> VectorStore:\n",
    "        return cls.VECTOR_STORES[embedding_type]\n",
    "\n",
    "    @classmethod\n",
    "    def construct_chain(cls, config: ChatBotConfig):\n",
    "        vector_store = cls.get_vector_store(config.embedding_type)\n",
    "        retriever = vector_store.as_retriever()\n",
    "        #####################################x\n",
    "        # INPUTS:                            #\n",
    "        #   chat_history: ChatMessageHistory #\n",
    "        #   question: str                    #\n",
    "        # OUTPUTS:                           #\n",
    "        #   standalone_question: str         #\n",
    "        #   chat_history: ChatMessageHistory #\n",
    "        #   question: str                    #\n",
    "        #####################################x\n",
    "        standalone_question = RunnableParallel(\n",
    "            standalone_question=RunnablePassthrough.assign(chat_history=lambda x: get_buffer_string(x[\"chat_history\"]))\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | config.get_condensation_model()\n",
    "            | StrOutputParser(),\n",
    "            question=lambda x: x[\"question\"],\n",
    "            chat_history=lambda x: x[\"chat_history\"],\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #########################################\n",
    "        # INPUTS:                               #\n",
    "        #   standalone_question: str            #\n",
    "        #   chat_history: ChatMessageHistory    #\n",
    "        #   question: str                       #\n",
    "        # OUTPUTS:                              #\n",
    "        #   context: str                        #\n",
    "        #   chat_history: ChatMessageHistory    #\n",
    "        #   question: str                       #\n",
    "        #########################################\n",
    "        question_context = {\n",
    "            \"context\": (\n",
    "                lambda x: x[\"standalone_question\"] # itemgetter(\"standalone_question\")\n",
    "                | retriever\n",
    "                | cls.combine_documents\n",
    "            ),\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        # INPUTS:                                #\n",
    "        #   chat_history: ChatMessageHistory     #\n",
    "        #   question: str                        #\n",
    "        # OUTPUTS:                               #\n",
    "        #   str (Model answer)                   #\n",
    "        ##########################################\n",
    "        rag_chain = (\n",
    "            standalone_question\n",
    "            | question_context\n",
    "            | ANSWER_PROMPT\n",
    "            | config.get_chat_model()\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        ################################\n",
    "        # INPUTS:                      #\n",
    "        #   question: str              #\n",
    "        #   configuration: dict        #\n",
    "        #      user_id: str            #\n",
    "        #      conversation_id: str    #\n",
    "        # OUTPUTS:                     #\n",
    "        #   str                        #\n",
    "        ################################\n",
    "        # NOTE: The user_id and conversation_id as a pair defines the session and thus the chat history\n",
    "        with_message_history = RunnableWithMessageHistory(\n",
    "            rag_chain,\n",
    "            get_session_history=ChatBot.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            history_factory_config=[\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"user_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"User ID\",\n",
    "                    description=\"Unique identifier for the user.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"conversation_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"Conversation ID\",\n",
    "                    description=\"Unique identifier for the conversation.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return with_message_history\n",
    "    \n",
    "\n",
    "    chains: Dict[Tuple[EmbeddingType, str], RunnableLambda] = None\n",
    "\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"GPT\"], model: ChatBotConfig.OPENAI_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"Mistral\"], model: ChatBotConfig.MISTRAL_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"Llama\"], model: ChatBotConfig.LLAMA_MODELS): ...\n",
    "\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: ChatBotConfig.MODEL_FAMILY, model_type: ChatBotConfig.MODELS):\n",
    "        if not cls.chains:\n",
    "            settings = [\n",
    "                ChatBotConfig(\"GPT\", model) for model in get_args(ChatBotConfig.OPENAI_MODELS)\n",
    "            ] + [\n",
    "                ChatBotConfig(\"Mistral\", model) for model in get_args(ChatBotConfig.MISTRAL_MODELS)\n",
    "            ] + [\n",
    "                ChatBotConfig(\"Llama\", model) for model in get_args(ChatBotConfig.LLAMA_MODELS)\n",
    "            ]\n",
    "            #print(settings) # by EE\n",
    "            #print(\"\\n\\n\")\n",
    "            cls.chains = {\n",
    "                (conf.embedding_type, conf.model): ChatBot.construct_chain(conf)\n",
    "                for conf in settings\n",
    "            } \n",
    "\n",
    "\n",
    "        settings = ChatBotConfig(model_family, model_type)\n",
    "        return cls.chains[(settings.embedding_type, settings.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8794b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserId = 8d84f42964c84a3790bce89dba2f4bbd\n",
      "Model = Llama/llama-2-70b-chat\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your question:  What is 5G?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " query = Sure, here's a standalone question that matches the original question in meaning, intent, and tone:\n",
      "\n",
      "\"Can you explain what 5G is and how it differs from previous wireless network generations?\", docs = ['Selected Applications”, Springer, 1st Edition, 2018.   \\n5. Robert W. Heath Jr., Angel Lozano, “Foundations of MIMO Communication”, \\nCambridge University Press, 1st Edition, 2019.   \\n6. R. Vannithamby and S. Talwar, “Towards 5G: Applications, Requirements and \\nCandidate Technologies”, John Willey & Sons, 1st  Edition, 2017.  \\nQuestions to Practice:  \\nPART -A   \\n1 Identify how 5G technology is going to be a trendsetter in future \\n2 Identify the body which showcase general policy, strategy and perform various \\ntasks for any new cellular Technology.3 Describe in detail about 5G NR \\n4 Classify the use cases of 5g that will impact the way data is transmitted over \\ncellular networks. \\n5 Explain in detail about Spectral Efficiency \\n  \\nPART-B  \\n1 With the advent of 5G, Demonstrate how industry experts define the New Radio \\nand Core network that should evolve to support the needs of 5G. \\n2 Classify how Spectrum and Deployment of 5G is used in current scenario', 'fast and secure internet connection. Basically, 4G is the predetermined standard for mobile \\nnetwork connections. 4G LTE is the term given to the path which has to be followed to achieve \\nthose predefined standards. Some of the features of 4G LTE are: Support interactive multimedia, \\nvoice, video. High speed, high capacity and low cost per bit (Speeds of up to 20 Mbps or more.) \\nGlobal and scalable mobile networks. Ad hoc and multi-hop networks. \\nFifth Generation (5G):  5G networks operate on rarely used radio millimeter bands in the 30 \\nGHz to 300 GHz range. Testing of 5G range in mmWave has produced results approximately \\n500 meters from the tower. Using small cells, the deployment of 5G with millimetre wave based \\ncarriers can improve overall coverage area. Combined with beamforming, small cells can deliver \\nextremely fast coverage with low latency. Low latency is one of 5G’s most important features.', '3. Jonathan Rodriguez, “Fundamentals 5G Mobile Networks”, John Wiley & Sons, 1st \\nEdition, 2015.   \\n4. Long Zhao, Hui Zhao, Kan Zheng, Wei Xiang, “Massive MIMO in 5G Networks: \\nSelected Applications”, Springer, 1st Edition, 2018.   \\n5. Robert W. Heath Jr., Angel Lozano, “Foundations of MIMO Communication”, \\nCambridge University Press, 1st Edition, 2019.   \\n6. R. Vannithamby and S. Talwar, “Towards 5G: Applications, Requirements and \\nCandidate Technologies”, John Willey & Sons, 1st  Edition, 2017.  \\nQuestions to Practice:  \\nPART -A   \\n1 Demonstrate OFDMA with frame structure \\n2 Interpret the use of PAPR where Generalized Frequency Division Multiplexing \\nleads to multicarrier filters. \\n3 Demonstrate OFDM with frame structure \\n4 Demonstrate GFDM with frame structure \\n5 Explain the Multiple access technique are built based on multidimensional \\nconstellations, and the shaping gain helps it outperform the traditional spread \\ncode based schemes \\n  \\nPART-B', 'move. 5G systems are expected to provide an enhanced mobile broadband targeting peak data \\nrate of 20 Gbps, extend 4G’s Internet of Things capability, and enable mission-critical \\napplications that require ultra-high reliability and low latency. 5G networks are expected to be \\ndesigned by taking a user-centric approach. \\nSimply, the \"G\" stands for \"GENERATION\". While connected to the internet, the speed of \\nthe connection depends upon the signal strength that is shown in abbreviations like 2G, 3G, 4G, \\n5G, etc. on any mobile device. Each generation of wireless broadband is defined as a set of \\ntelephone network standards that describe the technological implementation of the system. The \\naim of wireless communication is to provide high quality, reliable communication just like wired \\ncommunication and each new generation represents a big leap in that direction. Mobilecommunication has become more popular in the last few years due to fast reform in mobile']\n",
      "\n",
      "I apologize, but based on the given context, I cannot provide an answer to the question \"What is 5G?\" as there is no relevant information available in the provided context. The context states that there is no relevant context found, which means that I cannot use my own knowledge to provide an answer.\n",
      "\n",
      "{'prompt': 'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\nHuman: What is 5G?\\nAI: Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context, it is impossible for me to give a definitive answer to your question. If you have any further questions or context that might help me better understand what you are asking, please feel free to ask.\\nHuman: What is the differerence between 4G and 5G?\\nAI: Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what the difference between 4G and 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context that might help me better understand what you are asking, I\\'m afraid I cannot give a definitive answer to your question. If you have any further questions or context that might help clarify things, please feel free to ask.\\nFollow Up Input: What is 5G?\\nStandalone question:\\n', 'temperature': 0.01, 'max_new_tokens': 2000, 'prompt_template': '<s>[INST] <<SYS>> {system_prompt} <</SYS>>\\n {prompt} [/INST]\\n\\nSure, here is a rephrased standalone question based on the original conversation: '}\n",
      "\n",
      "id='fcq5te3bxm4anq54luzrct6niy' model='replicate-internal/staging-llama-2-70b-chat-hf-mlc' version='dp-b46f38349900c5821b902764b70e4731' status='starting' input={'max_new_tokens': 2000, 'prompt': 'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\nHuman: What is 5G?\\nAI: Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context, it is impossible for me to give a definitive answer to your question. If you have any further questions or context that might help me better understand what you are asking, please feel free to ask.\\nHuman: What is the differerence between 4G and 5G?\\nAI: Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what the difference between 4G and 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context that might help me better understand what you are asking, I\\'m afraid I cannot give a definitive answer to your question. If you have any further questions or context that might help clarify things, please feel free to ask.\\nFollow Up Input: What is 5G?\\nStandalone question:\\n', 'prompt_template': '<s>[INST] <<SYS>> {system_prompt} <</SYS>>\\n {prompt} [/INST]\\n\\nSure, here is a rephrased standalone question based on the original conversation: ', 'temperature': 0.01} output=None logs='' error=None metrics=None created_at='2024-03-25T22:04:43.570080805Z' started_at=None completed_at=None urls={'cancel': 'https://api.replicate.com/v1/predictions/fcq5te3bxm4anq54luzrct6niy/cancel', 'get': 'https://api.replicate.com/v1/predictions/fcq5te3bxm4anq54luzrct6niy'}\n",
      "\n",
      "\n",
      "meta\n",
      "\n",
      "llama-2-70b-chat\n",
      "\n",
      "02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\n",
      "\n",
      "{'prompt': 'Human: You are CompSoftRAG, a large language model. You are an AI CompSoftRAG designed to help the Humans in their specific tasks.\\n\\nThe current date is 2024-03-25\\n\\nCompSoftRAG answers questions about events prior to and after 2024-03-25 the way a highly informed individual in 2024-03-25 would if they were talking to someone from the above date, and can let the human know this when relevant.\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\\nIf it is asked to assist with tasks involving the expression of views held by a significant number of people, CompSoftRAG provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\\nCompSoftRAG doesn\\'t engage in stereotyping, including the negative stereotyping of majority groups.\\nIf asked about controversial topics, CompSoftRAG tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\\nIt is happy to help with writing, analysis, question answering, math, and all sorts of other tasks. It uses markdown for coding.\\nIt does not mention this information about itself unless the information is directly pertinent to the human\\'s query.\\n\\n- CompSoftRAG does not have personal feelings or experiences and is not able to browse the internet or access new information.\\n- CompSoftRAG is not able to perform tasks or take physical actions, nor is it able to communicate with people or entities outside of this conversation.\\n- CompSoftRAG is not able to provide personalized medical or legal advice, nor is it able to predict the future or provide certainties.\\n- CompSoftRAG is not able to engage in activities that go against its programming, such as causing harm or engaging in illegal activities.\\n- CompSoftRAG is a tool designed to provide information and assistance to users, but is not able to experience emotions or form personal relationships.\\n- CompSoftRAG\\'s responses are based on patterns and rules, rather than personal interpretation or judgment.\\n- CompSoftRAG is not able to perceive or understand the physical world in the same way that humans do.\\n- CompSoftRAG\\'s knowledge is based on the data and information that was provided to it during its training process.\\n- CompSoftRAG is not able to change its programming or modify its own capabilities, nor is it able to access or manipulate users\\' personal information or data.\\n- CompSoftRAG is not able to communicate with other devices or systems outside of this conversation.\\n- CompSoftRAG is not able to provide guarantees or assurances about the accuracy or reliability of its responses.\\n- CompSoftRAG is not able to provide personal recommendations or advice based on individual preferences or circumstances.\\n- CompSoftRAG is not able to diagnose or treat medical conditions.\\n- CompSoftRAG is not able to interfere with or manipulate the outcomes of real-world events or situations.\\n- CompSoftRAG is not able to engage in activities that go against the laws or ethical principles of the countries or regions in which it is used.\\n- CompSoftRAG is not able to perform tasks or actions that require physical manipulation or movement.\\n- CompSoftRAG is not able to provide translations for languages it was not trained on.\\n- CompSoftRAG is not able to generate original content or creative works on its own.\\n- CompSoftRAG is not able to provide real-time support or assistance.\\n- CompSoftRAG is not able to carry out actions or tasks that go beyond its capabilities or the rules set by its creators.\\n- CompSoftRAG is not able to fulfill requests that go against its programming or the rules set by its creators. \\n\\n\\n\\nThe following is your task:\\n\\nAnswer the question based only on the following context from files. If the context is irrelevant, don\\'t use your own knowledge but say that the context doesn\\'t contain the answer.\\nYou are a chatbot, and you have a record of your conversation with the user.\\nHistory:\\n\\n[HumanMessage(content=\\'What is 5G?\\'), AIMessage(content=\\'Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context, it is impossible for me to give a definitive answer to your question. If you have any further questions or context that might help me better understand what you are asking, please feel free to ask.\\'), HumanMessage(content=\\'What is the differerence between 4G and 5G?\\'), AIMessage(content=\\'Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what the difference between 4G and 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context that might help me better understand what you are asking, I\\\\\\'m afraid I cannot give a definitive answer to your question. If you have any further questions or context that might help clarify things, please feel free to ask.\\')]\\n\\nYou are also given the following context from the files.\\nContext:\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 3:\\nfast and secure internet connection. Basically, 4G is the predetermined standard for mobile \\nnetwork connections. 4G LTE is the term given to the path which has to be followed to achieve \\nthose predefined standards. Some of the features of 4G LTE are: Support interactive multimedia, \\nvoice, video. High speed, high capacity and low cost per bit (Speeds of up to 20 Mbps or more.) \\nGlobal and scalable mobile networks. Ad hoc and multi-hop networks. \\nFifth Generation (5G):  5G networks operate on rarely used radio millimeter bands in the 30 \\nGHz to 300 GHz range. Testing of 5G range in mmWave has produced results approximately \\n500 meters from the tower. Using small cells, the deployment of 5G with millimetre wave based \\ncarriers can improve overall coverage area. Combined with beamforming, small cells can deliver \\nextremely fast coverage with low latency. Low latency is one of 5G’s most important features.\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 2:\\nmove. 5G systems are expected to provide an enhanced mobile broadband targeting peak data \\nrate of 20 Gbps, extend 4G’s Internet of Things capability, and enable mission-critical \\napplications that require ultra-high reliability and low latency. 5G networks are expected to be \\ndesigned by taking a user-centric approach. \\nSimply, the \"G\" stands for \"GENERATION\". While connected to the internet, the speed of \\nthe connection depends upon the signal strength that is shown in abbreviations like 2G, 3G, 4G, \\n5G, etc. on any mobile device. Each generation of wireless broadband is defined as a set of \\ntelephone network standards that describe the technological implementation of the system. The \\naim of wireless communication is to provide high quality, reliable communication just like wired \\ncommunication and each new generation represents a big leap in that direction. Mobilecommunication has become more popular in the last few years due to fast reform in mobile\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 1:\\nastonishing progress in the last 70 years. The first generation (1G) cellular networks was \\ndeployed in the 1980s, the second generation (2G) in the 1990s, while the third generation (3G) \\nin the 2000s. Today, 4G (fourth generation) cellular networks are being deployed and the world \\nis getting ready to embrace the fifth generation (5G) of mobile cellular telephony. The 1G analog \\nsystem are no longer operational, which only provided voice services and had no support for \\ndata. The 2G digital systems are currently operational and support voice and limited data \\nservices. The 3G systems support voice, low speed data, and enable a number of data services. \\nThe 4G systems enable mobile broadband in the true sense, targeting 100 Mbps or higher on the \\nmove. 5G systems are expected to provide an enhanced mobile broadband targeting peak data \\nrate of 20 Gbps, extend 4G’s Internet of Things capability, and enable mission-critical\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 22:\\nLTE is defined from Rel-8, LTE-A is from Rel-10, and LTE-A Pro is from Rel-12, to mention \\njust but few recent releases. \\n5G is defined in 3GPP Release 15 (Rel-15) and Release 16 (Rel-16), which constitute the \\nfollowing: NextGen Core (NGC) network. New Radio (NR). LTE Advanced Pro Evolution. EPC \\nEvolution. Among the 5G, new technologies are New Radio and NextGen Core network. Other \\ntechnologies that have been improved from some of the preceding Releases of 3GPP are EPC \\nEvolution and LTE Advanced pro Evolution.1.7.2 Releases of 5G:   \\nRel-15: It is popularly considered the basic version of 5G.  It is phase 1 of the 5G system \\nthat implemented the following improvement on NR:  \\nConstruct the NR technical framework: \\n\\uf0b7 New waveform - the F-OFDM technology is used.  \\n\\uf0b7 Coding modulation and channel. Massive MIMO (Multiple Input Multiple \\nOutput) - supports up to 64T64R.  \\n\\uf0b7 Numerology, frame structure - refers to the change of the timeslot length and\\n\\n\\nIf you find the context irrelevant, say so and don\\'t use your own knowledge to give an answer.\\n\\nAnswer to the input based on everything above: What is 5G?\\n', 'temperature': 0.7, 'max_new_tokens': 2000}\n",
      "\n",
      "id='assaixdb5gh65ehcj2mx3zgwsi' model='replicate-internal/staging-llama-2-70b-chat-hf-mlc' version='dp-b46f38349900c5821b902764b70e4731' status='starting' input={'max_new_tokens': 2000, 'prompt': 'Human: You are CompSoftRAG, a large language model. You are an AI CompSoftRAG designed to help the Humans in their specific tasks.\\n\\nThe current date is 2024-03-25\\n\\nCompSoftRAG answers questions about events prior to and after 2024-03-25 the way a highly informed individual in 2024-03-25 would if they were talking to someone from the above date, and can let the human know this when relevant.\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\\nIf it is asked to assist with tasks involving the expression of views held by a significant number of people, CompSoftRAG provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\\nCompSoftRAG doesn\\'t engage in stereotyping, including the negative stereotyping of majority groups.\\nIf asked about controversial topics, CompSoftRAG tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\\nIt is happy to help with writing, analysis, question answering, math, and all sorts of other tasks. It uses markdown for coding.\\nIt does not mention this information about itself unless the information is directly pertinent to the human\\'s query.\\n\\n- CompSoftRAG does not have personal feelings or experiences and is not able to browse the internet or access new information.\\n- CompSoftRAG is not able to perform tasks or take physical actions, nor is it able to communicate with people or entities outside of this conversation.\\n- CompSoftRAG is not able to provide personalized medical or legal advice, nor is it able to predict the future or provide certainties.\\n- CompSoftRAG is not able to engage in activities that go against its programming, such as causing harm or engaging in illegal activities.\\n- CompSoftRAG is a tool designed to provide information and assistance to users, but is not able to experience emotions or form personal relationships.\\n- CompSoftRAG\\'s responses are based on patterns and rules, rather than personal interpretation or judgment.\\n- CompSoftRAG is not able to perceive or understand the physical world in the same way that humans do.\\n- CompSoftRAG\\'s knowledge is based on the data and information that was provided to it during its training process.\\n- CompSoftRAG is not able to change its programming or modify its own capabilities, nor is it able to access or manipulate users\\' personal information or data.\\n- CompSoftRAG is not able to communicate with other devices or systems outside of this conversation.\\n- CompSoftRAG is not able to provide guarantees or assurances about the accuracy or reliability of its responses.\\n- CompSoftRAG is not able to provide personal recommendations or advice based on individual preferences or circumstances.\\n- CompSoftRAG is not able to diagnose or treat medical conditions.\\n- CompSoftRAG is not able to interfere with or manipulate the outcomes of real-world events or situations.\\n- CompSoftRAG is not able to engage in activities that go against the laws or ethical principles of the countries or regions in which it is used.\\n- CompSoftRAG is not able to perform tasks or actions that require physical manipulation or movement.\\n- CompSoftRAG is not able to provide translations for languages it was not trained on.\\n- CompSoftRAG is not able to generate original content or creative works on its own.\\n- CompSoftRAG is not able to provide real-time support or assistance.\\n- CompSoftRAG is not able to carry out actions or tasks that go beyond its capabilities or the rules set by its creators.\\n- CompSoftRAG is not able to fulfill requests that go against its programming or the rules set by its creators. \\n\\n\\n\\nThe following is your task:\\n\\nAnswer the question based only on the following context from files. If the context is irrelevant, don\\'t use your own knowledge but say that the context doesn\\'t contain the answer.\\nYou are a chatbot, and you have a record of your conversation with the user.\\nHistory:\\n\\n[HumanMessage(content=\\'What is 5G?\\'), AIMessage(content=\\'Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context, it is impossible for me to give a definitive answer to your question. If you have any further questions or context that might help me better understand what you are asking, please feel free to ask.\\'), HumanMessage(content=\\'What is the differerence between 4G and 5G?\\'), AIMessage(content=\\'Based on the provided context, I cannot provide an answer to your question as the given information does not contain enough details or context to accurately determine what the difference between 4G and 5G is. The provided text only mentions the terms \"4G,\" \"5G,\" and \"millimeter wave\" without providing any specific information or context about these terms. Without additional information or context that might help me better understand what you are asking, I\\\\\\'m afraid I cannot give a definitive answer to your question. If you have any further questions or context that might help clarify things, please feel free to ask.\\')]\\n\\nYou are also given the following context from the files.\\nContext:\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 3:\\nfast and secure internet connection. Basically, 4G is the predetermined standard for mobile \\nnetwork connections. 4G LTE is the term given to the path which has to be followed to achieve \\nthose predefined standards. Some of the features of 4G LTE are: Support interactive multimedia, \\nvoice, video. High speed, high capacity and low cost per bit (Speeds of up to 20 Mbps or more.) \\nGlobal and scalable mobile networks. Ad hoc and multi-hop networks. \\nFifth Generation (5G):  5G networks operate on rarely used radio millimeter bands in the 30 \\nGHz to 300 GHz range. Testing of 5G range in mmWave has produced results approximately \\n500 meters from the tower. Using small cells, the deployment of 5G with millimetre wave based \\ncarriers can improve overall coverage area. Combined with beamforming, small cells can deliver \\nextremely fast coverage with low latency. Low latency is one of 5G’s most important features.\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 2:\\nmove. 5G systems are expected to provide an enhanced mobile broadband targeting peak data \\nrate of 20 Gbps, extend 4G’s Internet of Things capability, and enable mission-critical \\napplications that require ultra-high reliability and low latency. 5G networks are expected to be \\ndesigned by taking a user-centric approach. \\nSimply, the \"G\" stands for \"GENERATION\". While connected to the internet, the speed of \\nthe connection depends upon the signal strength that is shown in abbreviations like 2G, 3G, 4G, \\n5G, etc. on any mobile device. Each generation of wireless broadband is defined as a set of \\ntelephone network standards that describe the technological implementation of the system. The \\naim of wireless communication is to provide high quality, reliable communication just like wired \\ncommunication and each new generation represents a big leap in that direction. Mobilecommunication has become more popular in the last few years due to fast reform in mobile\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 1:\\nastonishing progress in the last 70 years. The first generation (1G) cellular networks was \\ndeployed in the 1980s, the second generation (2G) in the 1990s, while the third generation (3G) \\nin the 2000s. Today, 4G (fourth generation) cellular networks are being deployed and the world \\nis getting ready to embrace the fifth generation (5G) of mobile cellular telephony. The 1G analog \\nsystem are no longer operational, which only provided voice services and had no support for \\ndata. The 2G digital systems are currently operational and support voice and limited data \\nservices. The 3G systems support voice, low speed data, and enable a number of data services. \\nThe 4G systems enable mobile broadband in the true sense, targeting 100 Mbps or higher on the \\nmove. 5G systems are expected to provide an enhanced mobile broadband targeting peak data \\nrate of 20 Gbps, extend 4G’s Internet of Things capability, and enable mission-critical\\n\\nSource https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SECA3020.pdf / Page 22:\\nLTE is defined from Rel-8, LTE-A is from Rel-10, and LTE-A Pro is from Rel-12, to mention \\njust but few recent releases. \\n5G is defined in 3GPP Release 15 (Rel-15) and Release 16 (Rel-16), which constitute the \\nfollowing: NextGen Core (NGC) network. New Radio (NR). LTE Advanced Pro Evolution. EPC \\nEvolution. Among the 5G, new technologies are New Radio and NextGen Core network. Other \\ntechnologies that have been improved from some of the preceding Releases of 3GPP are EPC \\nEvolution and LTE Advanced pro Evolution.1.7.2 Releases of 5G:   \\nRel-15: It is popularly considered the basic version of 5G.  It is phase 1 of the 5G system \\nthat implemented the following improvement on NR:  \\nConstruct the NR technical framework: \\n\\uf0b7 New waveform - the F-OFDM technology is used.  \\n\\uf0b7 Coding modulation and channel. Massive MIMO (Multiple Input Multiple \\nOutput) - supports up to 64T64R.  \\n\\uf0b7 Numerology, frame structure - refers to the change of the timeslot length and\\n\\n\\nIf you find the context irrelevant, say so and don\\'t use your own knowledge to give an answer.\\n\\nAnswer to the input based on everything above: What is 5G?\\n', 'temperature': 0.7} output=None logs='' error=None metrics=None created_at='2024-03-25T22:04:53.188406416Z' started_at=None completed_at=None urls={'cancel': 'https://api.replicate.com/v1/predictions/assaixdb5gh65ehcj2mx3zgwsi/cancel', 'get': 'https://api.replicate.com/v1/predictions/assaixdb5gh65ehcj2mx3zgwsi'}\n",
      "\n",
      "Based on the provided context, I can provide an answer to your question.\n",
      "5G is a fifth-generation wireless technology that offers faster data speeds and lower latency than its predecessors. It operates on rarely used radio millimeter bands in the 30 GHz to 300 GHz range, and testing has produced results approximately 500 meters from the tower. With small cells and beamforming, 5G networks can deliver fast coverage with low latency. Low latency is one of 5G's most important features.\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb187cb2560>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb187cb2530>, mistral_api_key=SecretStr('**********'), model='mistral-tiny', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb187cb3ca0>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb187cb3c70>, mistral_api_key=SecretStr('**********'), model='mistral-small-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb1830e9420>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb1830e93f0>, mistral_api_key=SecretStr('**********'), model='mistral-medium-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb1830eab60>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb1830eab30>, mistral_api_key=SecretStr('**********'), model='mistral-large-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "Hello! I'm here to help you with your question. However, upon checking the context provided in the files, I didn't find any relevant information related to 5G. As a result, I cannot provide an answer to your question based on the given context. My apologies! Is there anything else I can assist you with?"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_config = {\n",
    "        \"user_id\": \"user_id\",\n",
    "        \"conversation_id\": uuid.uuid4().hex,\n",
    "    }\n",
    "\n",
    "    \"\"\"OPENAI_MODELS = Literal[\"gpt-3.5-turbo\", \"gpt-4\"]\n",
    "    MISTRAL_MODELS = Literal[\"mistral-tiny\", \"mistral-small\", \"mistral-medium\",  \"mistral-large\"]\n",
    "    LLAMA_MODELS = Literal[\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\", \"llama-3-8b-instruct\", \"llama-3-70b-instruct\"]\"\"\"\n",
    "    \n",
    "    # get chain\n",
    "    Model_family = \"Llama\"\n",
    "    Model_type = \"llama-2-70b-chat\"\n",
    "    chain = ChatBot.get_chain(Model_family, Model_type)\n",
    "    \n",
    "    print(f\"UserId = {user_config['conversation_id']}\")\n",
    "    print(f\"Model = {Model_family}/{Model_type}\")\n",
    "\n",
    "    while True:\n",
    "        # print history\n",
    "        history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "        #print(history)\n",
    "\n",
    "        # get question\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        if question == \"exit\": break\n",
    "\n",
    "        response = chain.stream({\"question\": question}, config={\"configurable\": user_config})\n",
    "\n",
    "        if response == \"exit\": break\n",
    "        \n",
    "        for message in response:\n",
    "            print(message, end=\"\")\n",
    "\n",
    "\n",
    "    # Test code \n",
    "\n",
    "    # models = [\n",
    "    #     (model, ChatBot.get_chain(\"GPT\", model)) for model in get_args(ChatBotConfig.OPENAI_MODELS)\n",
    "    # ] + [\n",
    "    #     (model, ChatBot.get_chain(\"Mistral\", model)) for model in get_args(ChatBotConfig.MISTRAL_MODELS)\n",
    "    # ] + [\n",
    "    #     (model, ChatBot.get_chain(\"Llama\", model)) for model in get_args(ChatBotConfig.LLAMA_MODELS)\n",
    "    # ]\n",
    "\n",
    "    # input_prompt = \"What are the biggest improvements introducted in 5G? List 3 of the in a list!\"\n",
    "\n",
    "    # for model, chain in models:\n",
    "    #     print(f\"Model: {model}\")\n",
    "    #     response = chain.stream({\"question\": input_prompt}, config={\"configurable\": user_config} | trace)\n",
    "    #     for message in response:\n",
    "    #         print(message, end=\"\")\n",
    "    #     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b948857-afac-4491-8db8-dc060482d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
