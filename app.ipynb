{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8e8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Literal, Tuple, Union, get_args, overload\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccee622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b000b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import ConfigurableFieldSpec, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d7fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import RAG, EmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4734fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ANSWER_PROMPT, CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab28aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25218f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBotConfig:\n",
    "    MODEL_FAMILY = Literal[\"GPT\", \"Mistral\", \"Llama\"]\n",
    "\n",
    "    OPENAI_MODELS = Literal[\"gpt-3.5-turbo\", \"gpt-4\"]\n",
    "    MISTRAL_MODELS = Literal[\"mistral-tiny\", \"mistral-small\", \"mistral-medium\",  \"mistral-large\"]\n",
    "    __LLAMA_MODEL_VERSIONS = {\n",
    "        \"llama-2-7b-chat\": \"13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\",\n",
    "        \"llama-2-13b-chat\": \"f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "        \"llama-2-70b-chat\": \"02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    }\n",
    "    \n",
    "    LLAMA_MODELS = Literal[\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\"]\n",
    "    MODELS = Union[OPENAI_MODELS, MISTRAL_MODELS, LLAMA_MODELS]\n",
    "\n",
    "    def get_model_name(self, model_family: MODEL_FAMILY, model: MODELS) -> str:\n",
    "        match model_family:\n",
    "            case \"GPT\":\n",
    "                return model\n",
    "            case \"Mistral\":\n",
    "                match model:\n",
    "                    case \"mistral-tiny\":\n",
    "                        return \"mistral-tiny\"\n",
    "                    case \"mistral-small\":\n",
    "                        return \"mistral-small-latest\"\n",
    "                    case \"mistral-medium\":\n",
    "                        return \"mistral-medium-latest\"\n",
    "                    case \"mistral-large\":\n",
    "                        return \"mistral-large-latest\"\n",
    "                    case _: \n",
    "                        raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.MISTRAL_MODELS)}\")\n",
    "            \n",
    "            case \"Llama\":\n",
    "                return f\"meta/{model}:{ChatBotConfig.__LLAMA_MODEL_VERSIONS[model]}\"\n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"GPT\"], model: OPENAI_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"Mistral\"], model: MISTRAL_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(cls, model_family: Literal[\"Llama\"], model: LLAMA_MODELS): ...\n",
    "\n",
    "    def __init__(self, model_family: MODEL_FAMILY, model: MODELS):\n",
    "        self.model_family = model_family\n",
    "        self.model = self.get_model_name(model_family, model)\n",
    "\n",
    "        match model_family:\n",
    "            case \"GPT\":\n",
    "                self.embedding_type = EmbeddingType.OPEN_AI\n",
    "                if model not in get_args(ChatBotConfig.OPENAI_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.OPENAI_MODELS)}\")\n",
    "            case \"Mistral\":\n",
    "                self.embedding_type = EmbeddingType.MISTRAL\n",
    "                if model not in get_args(ChatBotConfig.MISTRAL_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.MISTRAL_MODELS)}\")\n",
    "            case \"Llama\":\n",
    "                self.embedding_type = EmbeddingType.SENTENCE_TRANSFORMER\n",
    "                if model not in get_args(ChatBotConfig.LLAMA_MODELS):\n",
    "                    raise ValueError(f\"Invalid model: {model}. Must be one of {get_args(ChatBotConfig.LLAMA_MODELS)}\")\n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    def get_condensation_model(self) -> Union[ChatOpenAI, ChatMistralAI, Replicate]:\n",
    "        match self.model_family:\n",
    "            case \"GPT\":\n",
    "                return ChatOpenAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Mistral\":\n",
    "                return ChatMistralAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Llama\":\n",
    "                return Replicate(\n",
    "                    model=self.model,\n",
    "                    model_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 2000},\n",
    "                )\n",
    "                \n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {self.model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")\n",
    "\n",
    "    def get_chat_model(self):\n",
    "        match self.model_family:\n",
    "            case \"GPT\":\n",
    "                return ChatOpenAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Mistral\":\n",
    "                return ChatMistralAI(\n",
    "                    model=self.model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000,\n",
    "                )\n",
    "\n",
    "            case \"Llama\":\n",
    "                return Replicate(\n",
    "                    model=self.model,\n",
    "                    model_kwargs={\"temperature\": 0.7, \"max_new_tokens\": 2000},\n",
    "                )\n",
    "                \n",
    "            case _: \n",
    "                raise ValueError(f\"Invalid model family: {self.model_family}. Must be one of {get_args(ChatBotConfig.MODEL_FAMILY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566909e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    VECTOR_STORES = {\n",
    "        EmbeddingType.OPEN_AI: RAG(embedding_type=EmbeddingType.OPEN_AI),\n",
    "        EmbeddingType.SENTENCE_TRANSFORMER: RAG(embedding_type=EmbeddingType.SENTENCE_TRANSFORMER),\n",
    "        EmbeddingType.MISTRAL: RAG(embedding_type=EmbeddingType.MISTRAL),\n",
    "    }\n",
    "\n",
    "    store: Dict[Tuple[str, str], BaseChatMessageHistory] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "        if (user_id, conversation_id) not in ChatBot.store:\n",
    "            ChatBot.store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "        return ChatBot.store[(user_id, conversation_id)]\n",
    "   \n",
    "    @staticmethod\n",
    "    def del_session_history(user_id: str, conversation_id: str) -> None:\n",
    "        if (user_id, conversation_id) in ChatBot.store:\n",
    "            del ChatBot.store[(user_id, conversation_id)]\n",
    "\n",
    "    @classmethod\n",
    "    def format_document(cls, doc: Document) -> str:\n",
    "        document_info = {\n",
    "            \"content\": doc.page_content,\n",
    "            \"source\": f\"{doc.metadata['source_file']} / Page {doc.metadata['page_num']}\",\n",
    "        }\n",
    "        return DOCUMENT_PROMPT.format(**document_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_documents(docs, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [ChatBot.format_document(doc) for doc in docs]\n",
    "        context = document_separator.join(doc_strings)\n",
    "        return context or \"No relevant context found.\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_vector_store(cls, embedding_type: EmbeddingType) -> VectorStore:\n",
    "        return cls.VECTOR_STORES[embedding_type]\n",
    "\n",
    "    @classmethod\n",
    "    def construct_chain(cls, config: ChatBotConfig):\n",
    "        vector_store = cls.get_vector_store(config.embedding_type)\n",
    "        retriever = vector_store.as_retriever()\n",
    "\n",
    "        #####################################x\n",
    "        # INPUTS:                            #\n",
    "        #   chat_history: ChatMessageHistory #\n",
    "        #   question: str                    #\n",
    "        # OUTPUTS:                           #\n",
    "        #   standalone_question: str         #\n",
    "        #   chat_history: str                #\n",
    "        #   question: str                    #\n",
    "        #####################################x\n",
    "        standalone_question = RunnableParallel(\n",
    "            standalone_question=RunnablePassthrough.assign(chat_history=lambda x: get_buffer_string(x[\"chat_history\"]))\n",
    "            | CONDENSE_QUESTION_PROMPT\n",
    "            | config.get_condensation_model()\n",
    "            | StrOutputParser(),\n",
    "            question=lambda x: x[\"question\"],\n",
    "            chat_history=lambda x: x[\"chat_history\"],\n",
    "        )\n",
    "        \n",
    "        #print(f'\\n\\nStandalone question: {standalone_question}\\nitemgetter: {itemgetter(\"standalone_question\")}\\n\\n') # by EE\n",
    "        \n",
    "        #########################################\n",
    "        # INPUTS:                               #\n",
    "        #   standalone_question: str            #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        # OUTPUTS:                              #\n",
    "        #   context: str                        #\n",
    "        #   chat_history: str                   #\n",
    "        #   question: str                       #\n",
    "        #########################################\n",
    "        question_context = {\n",
    "            \"context\": (\n",
    "                itemgetter(\"standalone_question\")\n",
    "                | retriever\n",
    "                | cls.combine_documents\n",
    "            ),\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "\n",
    "        print(f'\\n\\nQuestion_context: {question_context}\\n\\n') # by EE\n",
    "\n",
    "        ##########################################\n",
    "        # INPUTS:                                #\n",
    "        #   chat_history: ChatMessageHistory     #\n",
    "        #   question: str                        #\n",
    "        # OUTPUTS:                               #\n",
    "        #   str (Model answer)                   #\n",
    "        ##########################################\n",
    "        rag_chain = (\n",
    "            standalone_question\n",
    "            | question_context\n",
    "            | ANSWER_PROMPT\n",
    "            | config.get_chat_model()\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        ################################\n",
    "        # INPUTS:                      #\n",
    "        #   question: str              #\n",
    "        #   configuration: dict        #\n",
    "        #      user_id: str            #\n",
    "        #      conversation_id: str    #\n",
    "        # OUTPUTS:                     #\n",
    "        #   str                        #\n",
    "        ################################\n",
    "        # NOTE: The user_id and conversation_id as a pair defines the session and thus the chat history\n",
    "        with_message_history = RunnableWithMessageHistory(\n",
    "            rag_chain,\n",
    "            get_session_history=ChatBot.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            history_factory_config=[\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"user_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"User ID\",\n",
    "                    description=\"Unique identifier for the user.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"conversation_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"Conversation ID\",\n",
    "                    description=\"Unique identifier for the conversation.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return with_message_history\n",
    "    \n",
    "\n",
    "    chains: Dict[Tuple[EmbeddingType, str], RunnableLambda] = None\n",
    "\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"GPT\"], model: ChatBotConfig.OPENAI_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"Mistral\"], model: ChatBotConfig.MISTRAL_MODELS): ...\n",
    "\n",
    "    @overload\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: Literal[\"Llama\"], model: ChatBotConfig.LLAMA_MODELS): ...\n",
    "\n",
    "    @classmethod\n",
    "    def get_chain(cls, model_family: ChatBotConfig.MODEL_FAMILY, model: ChatBotConfig.MODELS):\n",
    "        if not cls.chains:\n",
    "            settings = [\n",
    "                ChatBotConfig(\"GPT\", model) for model in get_args(ChatBotConfig.OPENAI_MODELS)\n",
    "            ] + [\n",
    "                ChatBotConfig(\"Mistral\", model) for model in get_args(ChatBotConfig.MISTRAL_MODELS)\n",
    "            ] + [\n",
    "                ChatBotConfig(\"Llama\", model) for model in get_args(ChatBotConfig.LLAMA_MODELS)\n",
    "            ]\n",
    "            \n",
    "            cls.chains = {\n",
    "                (conf.embedding_type, conf.model): ChatBot.construct_chain(conf)\n",
    "                for conf in settings\n",
    "            } \n",
    "\n",
    "\n",
    "        settings = ChatBotConfig(model_family, model)\n",
    "        return cls.chains[(settings.embedding_type, settings.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5514e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_config = {\n",
    "        \"user_id\": \"user_id\",\n",
    "        \"conversation_id\": uuid.uuid4().hex,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58056991",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  What is 5G?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fb18806bf10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fb188068fa0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fb187cc1d20>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fb187cc31f0>, model_name='gpt-4', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb187cb2560>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb187cb2530>, mistral_api_key=SecretStr('**********'), model='mistral-tiny', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb187cb3ca0>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb187cb3c70>, mistral_api_key=SecretStr('**********'), model='mistral-small-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb1830e9420>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb1830e93f0>, mistral_api_key=SecretStr('**********'), model='mistral-medium-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| ChatMistralAI(client=<mistralai.client.MistralClient object at 0x7fb1830eab60>, async_client=<mistralai.async_client.MistralAsyncClient object at 0x7fb1830eab30>, mistral_api_key=SecretStr('**********'), model='mistral-large-latest', temperature=0.0, max_tokens=2000)\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standalone question: steps={'standalone_question': RunnableAssign(mapper={\n",
      "  chat_history: RunnableLambda(lambda x: get_buffer_string(x['chat_history']))\n",
      "})\n",
      "| PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, do not change details and include every relevant detail without chainging them.\\nKeep the question as close to the original as possible, but make sure it is a standalone question. Make sure the standalone question matches the original question in meaning, intent, tone, and is as close as possible to the original question (with the added context).\\nDON\\'T BE TALKATIVE, DON\\'T USE COURTEOUS PHRASES, JUST GIVE ME THE STANDALONE QUESTION REQUESTED.\"\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\\n')\n",
      "| Replicate(model='meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', model_kwargs={'temperature': 0.01, 'max_new_tokens': 2000}, replicate_api_token='*********')\n",
      "| StrOutputParser(), 'question': RunnableLambda(lambda x: x['question']), 'chat_history': RunnableLambda(lambda x: x['chat_history'])}\n",
      "itemgetter: operator.itemgetter('standalone_question')\n",
      "\n",
      "\n",
      "Hello! I'm here to help you with your question. However, upon checking the context provided in the files, I didn't find any relevant information related to 5G. As a result, I cannot provide an answer to your question based on the given context. My apologies! Is there anything else I can assist you with?"
     ]
    }
   ],
   "source": [
    "    \"\"\"trace = {\n",
    "        \"callbacks\": [\n",
    "            CallbackHandler(\n",
    "                secret_key=\"sk-lf-de74539a-7177-49a8-9c3e-50492e90d1a9\",\n",
    "                public_key=\"pk-lf-71d6d1c4-7a5b-4f46-b75c-cd8dad7e35ff\",\n",
    "                host=\"http://localhost:3000\",\n",
    "            )\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "    while True:\n",
    "        # print history\n",
    "        history = ChatBot.get_session_history(user_config[\"user_id\"], user_config[\"conversation_id\"])\n",
    "        #print(history)\n",
    "\n",
    "        # get question\n",
    "        question = input(\"Enter your question: \")\n",
    "        if question == \"exit\": break\n",
    "\n",
    "        # get chain\n",
    "        chain = ChatBot.get_chain(\"Llama\", \"llama-2-7b-chat\")\n",
    "        \n",
    "        response = chain.stream({\"question\": question}, config={\"configurable\": user_config})\n",
    "        \n",
    "        for message in response:\n",
    "            print(message, end=\"\")\n",
    "\n",
    "\n",
    "    # Test code \n",
    "\n",
    "    # models = [\n",
    "    #     (model, ChatBot.get_chain(\"GPT\", model)) for model in get_args(ChatBotConfig.OPENAI_MODELS)\n",
    "    # ] + [\n",
    "    #     (model, ChatBot.get_chain(\"Mistral\", model)) for model in get_args(ChatBotConfig.MISTRAL_MODELS)\n",
    "    # ] + [\n",
    "    #     (model, ChatBot.get_chain(\"Llama\", model)) for model in get_args(ChatBotConfig.LLAMA_MODELS)\n",
    "    # ]\n",
    "\n",
    "    # input_prompt = \"What are the biggest improvements introducted in 5G? List 3 of the in a list!\"\n",
    "\n",
    "    # for model, chain in models:\n",
    "    #     print(f\"Model: {model}\")\n",
    "    #     response = chain.stream({\"question\": input_prompt}, config={\"configurable\": user_config} | trace)\n",
    "    #     for message in response:\n",
    "    #         print(message, end=\"\")\n",
    "    #     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa330a1-8e1a-47cb-9b56-c43b12832077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
