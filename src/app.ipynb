{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033237df-c8f4-4c07-b0ac-32c378bdcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Iterable\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from chatbot import ChatBot, ModelFamily, ModelName\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87094516",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .env file is expected to be in the root directory of the project\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "GRADIO_USER = os.environ[\"GRADIO_USER\"]\n",
    "GRADIO_PASSWORD = os.environ[\"GRADIO_PASSWORD\"]\n",
    "\n",
    "model_by_families = ChatBot.get_models_by_families()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acaf865",
   "metadata": {},
   "source": [
    "## Prompt execution pipelines\n",
    "\n",
    "We are defining utility function for Gradio\n",
    "\n",
    "The functions convert from the text representation of the model name and family to the enums.\n",
    "\n",
    "Also we convert the history into the right format for the Chatbot Gradio component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def exec_prompt(\n",
    "    question: str, session_id: str, model_family: str, model_name: str\n",
    ") -> tuple[list[list[str]], str]:\n",
    "    \"\"\"\n",
    "    Performs the retrieval and feeds the relevant documents to the model as the context.\n",
    "\n",
    "    Two LLM calls are made, one to condense the prompt and create a standalone question\n",
    "    from the history and question to be used with the retriever, and the other to answer\n",
    "    the question based on the context.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be asked.\n",
    "        session_id (str): The ID of the chat session. This defines the history.\n",
    "        model_family (str): The model family.\n",
    "        model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[list[str]], str]: A tuple containing the history pairs and an empty string.\n",
    "    \"\"\"\n",
    "\n",
    "    model_family_kind = ModelFamily(model_family)\n",
    "    model_name_kind = ModelName((model_family_kind, model_name))\n",
    "    question = question or \"I have no question\"\n",
    "\n",
    "    chain = ChatBot.get_chain(model_name_kind)\n",
    "\n",
    "    # Automatically adds to history\n",
    "    _ = chain.invoke(\n",
    "        input={\n",
    "            \"question\": question,\n",
    "        },\n",
    "        config={\n",
    "            \"configurable\": {\n",
    "                \"user_id\": GRADIO_USER,\n",
    "                \"conversation_id\": session_id,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chat_history = ChatBot.get_session_history(GRADIO_USER, session_id)\n",
    "\n",
    "    history_pairs = []\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    return history_pairs, \"\"\n",
    "\n",
    "\n",
    "def exec_prompt_streaming(\n",
    "    question: str, session_id: str, model_family: str, model_name: str\n",
    ") -> Iterable[tuple[list[list[str]], str]]:\n",
    "    \"\"\"\n",
    "    Performs the retrieval and feeds the relevant documents to the model as the context.\n",
    "\n",
    "    Two LLM calls are made, one to condense the prompt and create a standalone question\n",
    "    from the history and question to be used with the retriever, and the other to answer\n",
    "    the question based on the context.\n",
    "    Out of the two calls, the second one is streamed back to the caller\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be asked.\n",
    "        session_id (str): The ID of the chat session. This defines the history.\n",
    "        model_family (str): The model family.\n",
    "        model_name (str): The name of the model.\n",
    "    \n",
    "    Yields:\n",
    "        Iterable[tuple[list[list[str]], str]]: A generator that yields a tuple containing the history pairs and an empty string.\n",
    "    \"\"\"\n",
    "\n",
    "    model_family_kind = ModelFamily(model_family)\n",
    "    model_name_kind = ModelName((model_family_kind, model_name))\n",
    "    question = question or \"I have no question\"\n",
    "\n",
    "    chain = ChatBot.get_chain(model_name_kind)\n",
    "\n",
    "    chat_history = ChatBot.get_session_history(GRADIO_USER, session_id)\n",
    "    response = chain.stream(\n",
    "        input={\n",
    "            \"question\": question,\n",
    "        },\n",
    "        config={\n",
    "            \"configurable\": {\n",
    "                \"user_id\": GRADIO_USER,\n",
    "                \"conversation_id\": session_id,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    history_pairs = []\n",
    "\n",
    "    for msg in chat_history.messages:\n",
    "        if msg.type == \"human\":\n",
    "            history_pairs.append([msg.content, \"\"])\n",
    "        elif msg.type == \"ai\":\n",
    "            history_pairs[-1][1] = msg.content\n",
    "\n",
    "    history_pairs.append([question, \"\"])\n",
    "    for res in response:\n",
    "        if res is not None:\n",
    "            history_pairs[-1][1] += res\n",
    "        yield history_pairs, \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ada7a2",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "\n",
    "def save_datapoint(*args):\n",
    "    callback.flag(args)  # type: ignore\n",
    "    gr.Info(\"Data point flagged for review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b70983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    session_id = gr.Textbox(\n",
    "        value=uuid.uuid4,\n",
    "        interactive=False,\n",
    "        visible=False,\n",
    "    )\n",
    "\n",
    "    # Gradio UI\n",
    "    gr.Markdown(\"# Component Soft 5G RAG Demo\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_family = gr.Dropdown(\n",
    "            choices=list(model_by_families.keys()),\n",
    "            label=\"Model family\",\n",
    "            value=\"OpenAI GPT\",\n",
    "        )\n",
    "\n",
    "        model_name = gr.Dropdown(\n",
    "            list(model_by_families[model_family.value]),\n",
    "            label=\"Model\",\n",
    "            value=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"ComponentSoft_5G_RAG\",\n",
    "            height=400,\n",
    "            show_copy_button=True,\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Question\", value=\"What is 5G?\")\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(value=\"Answer\")\n",
    "        submit_btn_streaming = gr.Button(value=\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "\n",
    "    gr.Examples(\n",
    "        [\n",
    "            \"What is 5G?\",\n",
    "            \"What are its main adventages compared to 4G?\",\n",
    "            \"What frequencies does it use?\",\n",
    "            \"Which organisations are responsible for its standardization?\",\n",
    "            \"What is OFDMA?\",\n",
    "            \"What is the difference between OFDMA and OFDM?\",\n",
    "            \"What are the main components of 5G core networks?\",\n",
    "            \"What were the design principles of Massive MTC?\",\n",
    "        ],\n",
    "        prompt,\n",
    "    )\n",
    "\n",
    "    model_family.change(\n",
    "        fn=lambda family: gr.Dropdown(\n",
    "            choices=list(model_by_families[family]),\n",
    "            label=\"Model\",\n",
    "            value=model_by_families[family][0],\n",
    "            interactive=True,\n",
    "        ),\n",
    "        inputs=model_family,\n",
    "        outputs=model_name,\n",
    "    )\n",
    "\n",
    "    submit_btn_streaming.click(\n",
    "        exec_prompt_streaming,\n",
    "        inputs=[prompt, session_id, model_family, model_name],\n",
    "        outputs=[chatbot, prompt],\n",
    "    )\n",
    "\n",
    "    submit_btn_nostreaming.click(\n",
    "        exec_prompt,\n",
    "        inputs=[prompt, session_id, model_family, model_name],\n",
    "        outputs=[chatbot, prompt],\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda session_id: ChatBot.del_session_history(GRADIO_USER, session_id),\n",
    "        inputs=session_id,\n",
    "        preprocess=False,\n",
    "    )\n",
    "\n",
    "    flag_btn.click(\n",
    "        fn=save_datapoint,  # type: ignore\n",
    "        inputs=[model_family, model_name, chatbot],\n",
    "        preprocess=False,\n",
    "    )\n",
    "\n",
    "    callback.setup([model_family, model_name, chatbot], \"../flagged_data_points\")\n",
    "\n",
    "# demo.launch()\n",
    "demo.launch(share=True)\n",
    "# demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(GRADIO_USER, GRADIO_PASSWORD), max_threads=20, show_error=True, favicon_path=\"../data/favicon.ico\", state_session_capacity=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5548e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
